{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) using New Hampshire Case Law\n",
    "*With IBM Granite Models*\n",
    "\n",
    "The [New Hampshire Case Law Dataset](https://huggingface.co/datasets/free-law/nh) comes from the Caselaw Access Project via Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this notebook\n",
    "This notebook contains instructions for performing Retrieval Augumented Generation (RAG). RAG is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.\n",
    "\n",
    "RAG use cases include:\n",
    "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
    "- Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
    "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Initial setup:\n",
    "  - Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages using WatsonX, and store them in a vector database.\n",
    "- Upon each user query:\n",
    "  - Retrieve relevant passages from the database. In this recipe, we using an embedding of the query to retrieve semantically similar passages.\n",
    "  - Generate a response by feeding retrieved passage into a large language model, along with the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To get started, you'll need:\n",
    "* A [Replicate account](https://replicate.com/) and API token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "Granite Kitchen comes with a bundle of dependencies that are required for notebooks. See the list of packages in its [`setup.py`](https://github.com/ibm-granite-community/granite-kitchen/blob/main/setup.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"langchain_community<0.3.0\" \\\n",
    "    replicate \\\n",
    "    langchain-huggingface \\\n",
    "    langchain-milvus \\\n",
    "    datasets \\\n",
    "    transformers \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting System Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model to use for generating embedding vectors from text.\n",
    "\n",
    "To use a model from a provider other than Huggingface, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/utils/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Vector Database\n",
    "\n",
    "Specify the database to use for storing and retrieving embedding vectors.\n",
    "\n",
    "To connect to a vector database other than Milvus, substitute this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/utils/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "import uuid\n",
    "\n",
    "db_file = f\"/tmp/milvus_{str(uuid.uuid4())[:8]}.db\"\n",
    "print(f\"The vector database will be saved to {db_file}\")\n",
    "\n",
    "vector_db = Milvus(embedding_function=embeddings_model, connection_args={\"uri\": db_file}, auto_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Choose your LLM\n",
    "The LLM will be used for answering the question, given the retrieved text.\n",
    "\n",
    "Follow the instructions in [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/cee1513c77429d7ddbf0e5a49b29b7bc9ca0d996/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb), selecting a Granite Code model from the [`ibm-granite`](https://replicate.com/ibm-granite) org.\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "get_env_var(\"REPLICATE_API_TOKEN\")\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-3.0-8b-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tokenizer used by your chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.0-8b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the Data\n",
    "\n",
    "We will use a New Hampshire case law dataset to help the model answer questions about NH laws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the documents\n",
    "\n",
    "Download the [New Hampshire CAP Caselaw](https://huggingface.co/datasets/free-law/nh) dataset from HuggingFace using the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "# Load the documents from the dataset\n",
    "loader = HuggingFaceDatasetLoader(\"free-law/nh\", page_content_column=\"text\")\n",
    "documents = loader.load()\n",
    "print(\"Document Count: \" + str(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add metadata to the documents\n",
    "\n",
    "Add the `source` field, which is used below, to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc.metadata['source'] = doc.metadata['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents[:1]:\n",
    "    print(doc.metadata, \"\\n\")\n",
    "    print(doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Document Database\n",
    "\n",
    "We'll use the caselaw document database to retrieve the full text of the cases by case id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the database file and document table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the json objects in a sqlite database, keyed by id\n",
    "import sqlite3, os, json\n",
    "\n",
    "# remove database file if exists\n",
    "if os.path.isfile('data.db'):\n",
    "    os.remove('data.db')\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# create the table if it doesn't exist. include id, text, and size\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS data\n",
    "             (id INTEGER PRIMARY KEY UNIQUE,\n",
    "              metadata TEXT,\n",
    "              text TEXT,\n",
    "              char_count INTEGER)''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the documents into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    id = doc.metadata[\"id\"]\n",
    "    c.execute(\"INSERT INTO data (id, metadata, text, char_count) VALUES (?,?,?,?)\", (id, json.dumps(doc.metadata), doc.page_content, doc.metadata[\"char_count\"]))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"SELECT count(*) FROM data\")\n",
    "doc_count = c.fetchone()[0]\n",
    "print(f\"Document count: {doc_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vector Database\n",
    "\n",
    "In this example, we take the caselaw text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "Split the document into text segments that can fit into the model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(\"Chunk Count: \" + str(len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for i in range(1):\n",
    "    print(chunks[i].page_content)\n",
    "    print(json.dumps(chunks[i].metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Populate the vector database\n",
    "\n",
    "NOTE: Population of the vector database may take a few minutes depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_db.add_documents(chunks)\n",
    "print(\"Document IDs: \" + str(ids[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create query text\n",
    "\n",
    "Here we use a topic of NH law to query into the vector database for relevant cases. Because we will consider one case at a time (due to context length restrictions), phrase the query to consider a single case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize this court case about the Suspension and Expulsion of Pupils, using the IRAC framework (Issue, Rule, Application, Conclusion).\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the vector database\n",
    "\n",
    "Query the vector database for cases related to the law. Similar documents are found by proximity of the embedded vector in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  # the number of docs to retrieve\n",
    "docs_with_score = vector_db.similarity_search_with_score(query, k=k)\n",
    "\n",
    "# Get a unique set of docs.\n",
    "docs = []\n",
    "doc_ids = {}\n",
    "for doc, score in docs_with_score:\n",
    "    # print(doc.metadata[\"name_abbreviation\"])\n",
    "    # print(score)\n",
    "    id = doc.metadata[\"id\"]\n",
    "    if id not in doc_ids:\n",
    "        docs.append(doc)\n",
    "        print(id, \" - \", doc.metadata[\"name_abbreviation\"])\n",
    "        doc_ids[id] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the document database\n",
    "\n",
    "Get the full text of the first case found by the vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of unique doc ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique doc ids.\n",
    "docs_ids_seen = set()\n",
    "uq_docs = [doc for doc in docs if not (doc.metadata[\"id\"] in docs_ids_seen or docs_ids_seen.add(doc.metadata[\"id\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a number of cases.\n",
    "cases = []\n",
    "\n",
    "for doc in docs:\n",
    "    case_id = doc.metadata[\"id\"]\n",
    "    case_short_name = doc.metadata[\"name_abbreviation\"]\n",
    "\n",
    "    c.execute(\"SELECT text FROM data where id = ?\", (case_id,))\n",
    "    case_text = c.fetchone()[0]\n",
    "    case_length = len(tokenizer.tokenize(case_text))\n",
    "\n",
    "    # For this recipe, only consider cases that can fit in the 4k context window (along with the 512 token output).\n",
    "    if case_length < 3500:\n",
    "        cases.append((case_id, case_short_name, case_text))\n",
    "        print(f\"Case Id: {case_id}\")\n",
    "        print(f\"Case Name: {case_short_name}\")\n",
    "        print(f\"Case Length: {case_length} tokens\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the Chat Prompt\n",
    "\n",
    "Build a chat prompt template with the law and the retrieved case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant with legal expertise. Answer the question based only on the following text from a NH court case. Do not include any other court cases. \\n\\n{case_text}\"\n",
    ")\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask questions of the retrieved case in relation to the law.\n",
    "\n",
    "Answer the question about each related case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in cases[:2]:\n",
    "    (case_id, case_short_name, case_text) = case\n",
    "    response = rag_chain.invoke(input = {\"input\": query, \"case_text\": case_text})\n",
    "    print(f\"Case {case_id}: {case_short_name}\\n\")\n",
    "    print(response, \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
